<?xml version="1.0"?>
<doc>
<assembly>
<name>
LibOptimization
</name>
</assembly>
<members>
<member name="M:LibOptimization.Optimization.clsOptPatternSearch.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Double,System.Double)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default 5 =&gt; -5 to 5)</param>
	<param name="ai_maxIteration">Optional:Iteration(default 20000)</param>
	<param name="ai_eps">Optional:Eps(default:1e-8)</param>
	<param name="ai_steplength">Optinal:step length(default 0.6)</param>
	<param name="ai_coeffShrink">Optional:Shrink coeffcient(default:2.0)</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.Init">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.Init(System.Double[])">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.InitInner">
	<summary>
 Init parameter
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.DoIteration(System.Int32)">
	<summary>
 Do optimization
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.MakeExploratoryMoves(LibOptimization.Optimization.clsPoint,System.Double)">
	<summary>
 Exploratory Move
 </summary>
	<param name="ai_base">Base point</param>
	<param name="ai_stepLength">Step</param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.MakePatternMove(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint)">
	<summary>
 Pattern Move
 </summary>
	<param name="ai_previousBasePoint"></param>
	<param name="ai_base"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptPatternSearch.Result">
	<summary>
 Result
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.GetLastErrorInfomation">
	<summary>
 Get recent error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptPatternSearch.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptPatternSearch">
	<summary>
 Hooke-Jeeves Pattern Search Method
 </summary>
	<remarks>
 Features:
  -Derivative free optimization algorithm.
 
 Reffrence:
 Hooke, R. and Jeeves, T.A., ""Direct search" solution of numerical and statistical problems", Journal of the Association for Computing Machinery (ACM) 8 (2), pp212–229.
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 </remarks>
</member><member name="T:LibOptimization.MathUtil.clsShoddyVector.VectorDirection">
	<summary>
 Vector direction.
 Row vector or Column vector.
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.#ctor">
	<summary>
 Default construcotr
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.#ctor(LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Copy constructor
 </summary>
	<param name="ai_base"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.#ctor(System.Int32,LibOptimization.MathUtil.clsShoddyVector.VectorDirection)">
	<summary>
 Constructor
 </summary>
	<param name="ai_dim"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.#ctor(System.Collections.Generic.List{System.Double},LibOptimization.MathUtil.clsShoddyVector.VectorDirection)">
	<summary>
 Constructor
 </summary>
	<param name="ai_val"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.#ctor(System.Double[],LibOptimization.MathUtil.clsShoddyVector.VectorDirection)">
	<summary>
 Constructor
 </summary>
	<param name="ai_val"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Implicit(System.Double[])~LibOptimization.MathUtil.clsShoddyVector">
	<summary>
 Type convert
 </summary>
	<param name="ai_ar"></param>
	<returns></returns>
	<remarks>
 double() -&gt; clsShoddyVector
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Addition(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Add
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Subtraction(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Diff
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.InnerProduct(LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Product(Inner product, dot product)
 </summary>
	<param name="ai_source"></param>
	<returns></returns>
	<remarks>
 a dot b = |a||b|cos(theta)
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Multiply(System.Double,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Product
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Multiply(LibOptimization.MathUtil.clsShoddyVector,System.Double)">
	<summary>
 Product
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Division(System.Double,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Divide
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Division(LibOptimization.MathUtil.clsShoddyVector,System.Double)">
	<summary>
 Divide
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.op_Exponent(LibOptimization.MathUtil.clsShoddyVector,System.Double)">
	<summary>
 Power(exponentiation)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.SetList(System.Collections.Generic.List{System.Double})">
	<summary>
 Set List(Of double)
 </summary>
	<param name="ai_list"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.ToMatrix">
	<summary>
 To Matrix
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.NormL1">
	<summary>
 Norm
 </summary>
	<returns></returns>
	<remarks>
 |x|1
 Refference:
 皆本晃弥, "C言語による「数値計算入門」～ 解法・アルゴリズム・プログラム ～", サイエンス社 2008年 初版第4刷, pp28-32
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.NormL2">
	<summary>
 Norm2
 </summary>
	<returns></returns>
	<remarks>
 ||x||
 Refference:
 皆本晃弥, "C言語による「数値計算入門」～ 解法・アルゴリズム・プログラム ～", サイエンス社 2008年 初版第4刷, pp28-32
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.NormMax">
	<summary>
 NormMax
 </summary>
	<returns></returns>
	<remarks>
 |x|max
 Refference:
 皆本晃弥, "C言語による「数値計算入門」～ 解法・アルゴリズム・プログラム ～", サイエンス社 2008年 初版第4刷, pp28-32
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.T">
	<summary>
 Transpose
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.PrintValue(System.Int32)">
	<summary>
 For Debug
 </summary>
	<param name="ai_preci"></param>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyVector.RawVector">
	<summary>
 Accessor 
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyVector.Direction">
	<summary>
 Vector direction
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyVector.IsSameDimension(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 CheckDimension
 </summary>
	<param name="ai_vec1"></param>
	<param name="ai_vec2"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.MathUtil.clsShoddyVector">
	<summary>
 Vector class
 </summary>
	<remarks>
 Inherits List(of double)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchTest.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchTest.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchTest">
	<summary>
 x1^3 + x2^3 - 0*x1*x2 + 27
 </summary>
	<remarks>
	</remarks>
</member><member name="P:LibOptimization.My.Resources.Resources.ResourceManager">
	<summary>
  Returns the cached ResourceManager instance used by this class.
</summary>
</member><member name="P:LibOptimization.My.Resources.Resources.Culture">
	<summary>
  Overrides the current thread's CurrentUICulture property for all
  resource lookups using this strongly typed resource class.
</summary>
</member><member name="T:LibOptimization.My.Resources.Resources">
	<summary>
  A strongly-typed resource class, for looking up localized strings, etc.
</summary>
</member><member name="M:LibOptimization.Optimization.absOptimization.Init">
	<summary>
 Initialize parameter
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.absOptimization.DoIteration(System.Int32)">
	<summary>
 Do Iteration
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>true:Stopping Criterion. false:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.absOptimization.Result">
	<summary>
 Result
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.absOptimization.IsRecentError">
	<summary>
 Recent Error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.absOptimization.IterationCount">
	<summary>
 Iteration count 
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.absOptimization.ObjectiveFunction">
	<summary>
 Objective Function
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.absOptimization.Random">
	<summary>
 Random object
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.absOptimization">
	<summary>
 Abstarct optimization Class
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Boolean,System.Int32,System.Int32,LibOptimization.Optimization.clsOptRealGAREX.REX_RANDMODE,System.Int32)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default: 10 =&gt; -10 to 10)</param>
	<param name="ai_generation">Optional:Generation(Default: 10000)</param>
	<param name="ai_eps">Optional:Eps(Default:1e-8)</param>
	<param name="ai_isUseEps">Optional:Use criterion(Default: true)</param>
	<param name="ai_populationSize">Optional:Population size(0 is n*8)</param>
	<param name="ai_parentsSize">Optional:Parents size(0 is n+1)</param>
	<param name="ai_REXRandomMode">Optional:REX(phi) Uniform or ND(default: Uniform)</param>
	<param name="ai_childsSize">Optional:Childs size(0 is n*6)</param>
	<remarks>
 "n" is function dimension.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.Init">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.DoIteration(System.Int32)">
	<summary>
 Do Iteration
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.UseEliteStrategy(System.Double)">
	<summary>
 using Elite Strategy
 </summary>
	<param name="ai_density">density</param>
	<remarks>
 Elite strategy
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.SelectParent(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint},System.Int32)">
	<summary>
 Select Parent
 </summary>
	<param name="ai_population"></param>
	<param name="ai_parentSize"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.CrossOverREX(LibOptimization.Optimization.clsOptRealGAREX.REX_RANDMODE,System.Int32,System.Collections.Generic.List{System.Collections.Generic.KeyValuePair{System.Int32,LibOptimization.Optimization.clsPoint}})">
	<summary>
 REX(Real-coded Ensemble Crossover)
 </summary>
	<param name="ai_randomMode"></param>
	<param name="ai_childNum">ChildNum</param>
	<param name="ai_parents"></param>
	<returns></returns>
	<remarks>
 REX(U, n+k) -&gt; U is UniformRandom
 REX(N, n+k) -&gt; N is NormalDistribution
 "n+k" is parents size.
 </remarks>
</member><member name="P:LibOptimization.Optimization.clsOptRealGAREX.Result">
	<summary>
 Best result
 </summary>
	<returns>Best point class</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptRealGAREX.AllResult">
	<summary>
 All Result
 </summary>
	<value></value>
	<returns></returns>
	<remarks>
 for Debug, Experiment
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGAREX.IsCriterion(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint)">
	<summary>
 Check Criterion
 </summary>
	<param name="ai_best"></param>
	<param name="ai_worst"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptRealGAREX">
	<summary>
 Real-coded Genetic Algorithm
 SPX + JGG
 </summary>
	<remarks>
 Features:
  -Derivative free optimization algorithm.
  -Cross over algorithm is REX(Real-coded Ensemble Cross over).
  -Alternation of generation algorithm is JGG.
 
 Refference:
 小林重信, "実数値GAのフロンティア"，人工知能学会誌 Vol. 24, No. 1, pp.147-162 (2009)
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchPowellFunction.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchPowellFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchPowellFunction">
	<summary>
 Benchmark function
 Powell function
 </summary>
	<remarks>
 Minimum:
  x = {0,0,0,0}
  f(x) = 0
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRidgeFunction.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRidgeFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchRidgeFunction">
	<summary>
 Benchmark function
 Ridge Function
 </summary>
	<remarks>
 Minimum:
  x = {0,...,0}
 
 Referrence:
 http://mikilab.doshisha.ac.jp/dia/research/pdga/archive/doc/ga2k_performance.pdf
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchBoothFunction.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchBoothFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchBoothFunction">
	<summary>
 Benchmark function
 Booth Function
 </summary>
	<remarks>
 Minimum:
  x = {1,3}
 
 Referrence:
 http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar_files/TestGO_files/Page816.htm
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchEllipsoidFunction.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<param name="ai_dim">Set dimension</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchEllipsoidFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchEllipsoidFunction">
	<summary>
 Benchmark function
 Ellipsoid function
 </summary>
	<remarks>
 Minimum:
  x = {0,...,0}
 
 Refference:
 [1]小林重信, "実数値GAのフロンティア"，人工知能学会誌 Vol. 24, No. 1, pp.147-162 (2009)
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsException.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsException.#ctor(LibOptimization.MathUtil.clsException.Series)">
	<summary>
 Constructor
 </summary>
	<param name="ai_series"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsException.#ctor(LibOptimization.MathUtil.clsException.Series,System.String)">
	<summary>
 Constructor
 </summary>
	<param name="ai_series"></param>
	<param name="ai_msg"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction1.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction1.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction1">
	<summary>
 Benchmark function
 De Jong’s function 1 (Sphere Function)
 </summary>
	<remarks>
 Minimum:
  x = {0, 0, 0}
 Range
  -5.12 ~ 5.12 
 Refference:
  De Jong, K. A., "Analysis of the Behavior of a Class of Genetic Adaptive Systems", PhD dissertation, The University of Michigan, Computer and Communication Sciences Department (1975)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchSphere.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<param name="ai_dim">Set dimension</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchSphere.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchSphere">
	<summary>
 Benchmark function
 Sphere Function
 </summary>
	<remarks>
 Minimum:
  x = {0,...,0}
  f(x) = 0
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.#ctor(LibOptimization.Optimization.clsPoint)">
	<summary>
 copy constructor
 </summary>
	<param name="ai_vertex"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.#ctor(LibOptimization.Optimization.absObjectiveFunction)">
	<summary>
 constructor
 </summary>
	<param name="ai_func"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Collections.Generic.List{System.Double})">
	<summary>
 constructor
 </summary>
	<param name="ai_func"></param>
	<param name="ai_vars"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double[])">
	<summary>
 constructor
 </summary>
	<param name="ai_func"></param>
	<param name="ai_vars"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.CompareTo(System.Object)">
	<summary>
 Compare(ICompareble)
 </summary>
	<param name="ai_obj"></param>
	<returns></returns>
	<remarks>
 larger Me than obj is -1. smaller Me than obj is 1.
 Equal is return to Zero
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.ReEvaluate">
	<summary>
 Re Evaluate
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsPoint.GetFunc">
	<summary>
 Get Function
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsPoint.Eval">
	<summary>
 EvaluateValue
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsPoint">
	<summary>
 Point Class
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Double)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default 5 =&gt; -5 to 5)</param>
	<param name="ai_maxIteration">Optional:Iteration(default 100)</param>
	<param name="ai_eps">Optional:Eps(default:1e-8)</param>
	<param name="ai_alpha">Optinal:update alpha(default 1.0)</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.Init">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.Init(System.Double[])">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.DoIteration(System.Int32)">
	<summary>
 Do Iteration
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptNewtonMethod.Result">
	<summary>
 Result
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.GetLastErrorInfomation">
	<summary>
 Get recent error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNewtonMethod.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptNewtonMethod">
	<summary>
 Newton Method
 </summary>
	<remarks>
 Features:
  -Use second derivertive.
  -Second order conversion.
 
 Refference:
 金谷健一, "これならわかる最適化数学－基礎原理から計算手法まで－", 共立出版株式会社 2007 初版第7刷, pp79-84 
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 
 Memo:
 最適化で微分を用いて求める手法のことを「勾配法」という。
 最大値を求めることを山登り法、最小値の場合は最急降下法とよばれる。
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction2.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction2.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction2">
	<summary>
 Benchmark function
 De Jong’s function 2 (2D Rosenblock Function)
 </summary>
	<remarks>
 Minimum:
  x = {1,1}
 Range
  -2.048 ~ 2.048
 Refference:
  De Jong, K. A., "Analysis of the Behavior of a Class of Genetic Adaptive Systems", PhD dissertation, The University of Michigan, Computer and Communication Sciences Department (1975)
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Double,System.Double,System.Double,System.Double)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default 5 =&gt; -5 to 5)</param>
	<param name="ai_maxIteration">Optional:Iteration(default 5000)</param>
	<param name="ai_eps">Optional:Eps(default:1e-6)</param>
	<param name="ai_coeffRefrection">Optional:Refrection coeffcient(default:1.0)</param>
	<param name="ai_coeffExpantion">Optional:Expantion coeffcient(default:2.0)</param>
	<param name="ai_coeffContraction">Optional:Contraction coeffcient(default:0.5)</param>
	<param name="ai_coeffShrink">Optional:Shrink coeffcient(default:2.0)</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.Init">
	<summary>
 Init
 </summary>
	<remarks>
 All vertexs are made at random.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.Init(System.Double[])">
	<summary>
 Init
 </summary>
	<param name="ai_initPoint">Set 1 vertex</param>
	<remarks>
 Other vertexs are made at random.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.Init(System.Double[][])">
	<summary>
 Init
 </summary>
	<param name="ai_initPoint"></param>
	<remarks>
 Set simplex
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.DoIteration(System.Int32)">
	<summary>
 Do optimization
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptNelderMead.Result">
	<summary>
 Best result
 </summary>
	<returns>Best point class</returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptNelderMead.AllResult">
	<summary>
 All Result
 </summary>
	<value></value>
	<returns></returns>
	<remarks>
 for Debug, Experiment
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.GetCentroid(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint})">
	<summary>
 Calc Centroid
 </summary>
	<param name="ai_vertexs"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.CalcRefrection(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint,System.Double)">
	<summary>
 Refrection
 </summary>
	<param name="ai_tgt">Target vertex</param>
	<param name="ai_base">Base vertex</param>
	<param name="ai_coeff">Expantion coeffcient. Recommned value 1.0</param>
	<returns></returns>
	<remarks>
 xr = (1 + alpha)¯x - p
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.CalcExpantion(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint,System.Double)">
	<summary>
 Expantion
 </summary>
	<param name="ai_tgt">Target vertex</param>
	<param name="ai_base">Base vertex</param>
	<param name="ai_coeff">Expantion coeffcient. Recommned value 2.0</param>
	<returns></returns>
	<remarks>
 xe = gamma * p + (1 - gamma)¯x
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.CalcContraction(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint,System.Double)">
	<summary>
 Contraction
 </summary>
	<param name="ai_tgt">Target vertex</param>
	<param name="ai_base">Base vertex</param>
	<param name="ai_coeff">Constraction coeffcient. Recommned value 0.5</param>
	<returns></returns>
	<remarks>
 xc = beta * p + (1 - beta)¯x
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.Shrink(System.Double)">
	<summary>
 Shrink(All point replace)
 </summary>
	<param name="ai_coeff">Shrink coeffcient.</param>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.IsConversion2(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint})">
	<summary>
 Check conversion
 </summary>
	<param name="ai_vertexs">All vertexs</param>
	<returns></returns>
	<remarks>
 Reffrence:
 William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
 "NUMRICAL RECIPIES 3rd Edition: The Art of Scientific Computing", Cambridge University Press 2007, pp505-506
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.GetLastErrorInfomation">
	<summary>
 Get recent error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMead.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptNelderMead">
	<summary>
 Nelder Mead Method
 </summary>
	<remarks>
 Features:
  -Derivative free optimization algorithm.
  -Also known as "Down hill simplex" or "simplex method".
  -Implementation according to the original paper.
 
 Reffrence:
 J.A.Nelder and R.Mead, "A Simplex Method for Function Minimization", The Computer Journal vol.7, 308–313 (1965)
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction3.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction3.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction3">
	<summary>
 Benchmark function
 De Jong’s function 3 (Step Function)
 </summary>
	<remarks>
 Minimum:
  x = {-5.12~-5, -5.12~-5, -5.12~-5, -5.12~-5, -5.12~-5}
 Range
  -5.12 ~ 5.12 
 Refference:
  De Jong, K. A., "Analysis of the Behavior of a Class of Genetic Adaptive Systems", PhD dissertation, The University of Michigan, Computer and Communication Sciences Department (1975)
 </remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor">
	<summary>
 Default construcotr
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Copy constructor
 </summary>
	<param name="ai_base"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(System.Int32,System.Boolean)">
	<summary>
 Constructor
 </summary>
	<param name="ai_dim"></param>
	<param name="ai_isIdentity">Make Identify matrix</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(System.Int32,System.Int32)">
	<summary>
 Constructor
 </summary>
	<param name="ai_rowSize"></param>
	<param name="ai_colSize"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(System.Collections.Generic.List{System.Collections.Generic.List{System.Double}})">
	<summary>
 Constructor
 </summary>
	<param name="ai_val"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(System.Double[][])">
	<summary>
 Constructor
 </summary>
	<param name="ai_val"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.#ctor(System.Collections.Generic.List{System.Double},LibOptimization.MathUtil.clsShoddyVector.VectorDirection)">
	<summary>
 Constructor
 </summary>
	<param name="ai_val"></param>
	<param name="ai_direction">Row or Col</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Addition(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Add(Matrix + Matrix)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Addition(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>s
 Add(Matrix + Vector)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Addition(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Add(Vector + Matrix)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Subtraction(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Diff
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Subtraction(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Diff(Matrix + Vector)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Subtraction(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Diff(Vector + Matrix)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Multiply(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Product( Matrix * Matrix )
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Multiply(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Product( Matrix * Vector )
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Multiply(LibOptimization.MathUtil.clsShoddyVector,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Product(Vector * Matrix)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Multiply(System.Double,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Product(value * Matrix)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.op_Multiply(LibOptimization.MathUtil.clsShoddyMatrix,System.Double)">
	<summary>
 Product(Matrix * value)
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.T">
	<summary>
 Transpose
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.Det(System.Boolean)">
	<summary>
 Determinant
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.Diag">
	<summary>
 Diagonal matrix
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.Inverse">
	<summary>
 Inverse
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.SwapRow(System.Int32,System.Int32)">
	<summary>
 Swap Row
 </summary>
	<param name="ai_sourceRowIndex"></param>
	<param name="ai_destRowIndex"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.SwapCol(System.Int32,System.Int32)">
	<summary>
 Swap Col
 </summary>
	<param name="ai_sourceColIndex"></param>
	<param name="ai_destColIndex"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.PrintValue(System.Int32)">
	<summary>
 For Debug
 </summary>
	<param name="ai_preci"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.ToVector">
	<summary>
 To Vector
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.IsSquare">
	<summary>
 正方行列判定
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyMatrix.RowCount">
	<summary>
 Get Row count
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyMatrix.ColCount">
	<summary>
 Get Collumn count
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyMatrix.Row(System.Int32)">
	<summary>
 Row accessor
 </summary>
	<param name="ai_rowIndex"></param>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyMatrix.Column(System.Int32)">
	<summary>
 Column accessor
 </summary>
	<param name="ai_colIndex"></param>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.MathUtil.clsShoddyMatrix.RawMatrix">
	<summary>
 To List(Of List(Of Double))
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.IsSameDimension(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyMatrix)">
	<summary>
 Check Dimension
 </summary>
	<param name="ai_source"></param>
	<param name="ai_dest"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.IsComputableMatrixVector(LibOptimization.MathUtil.clsShoddyMatrix,LibOptimization.MathUtil.clsShoddyVector)">
	<summary>
 Check Matrix Vector
 </summary>
	<param name="ai_matrix"></param>
	<param name="ai_vector"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.MathUtil.clsShoddyMatrix.CalcDeterminant(LibOptimization.MathUtil.clsShoddyMatrix,System.Int32,System.Boolean)">
	<summary>
 Determinant(Recursive)
 </summary>
	<param name="ai_clsMatrix"></param>
	<param name="ai_dim"></param>
	<param name="ai_isDebug"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.MathUtil.clsShoddyMatrix">
	<summary>
 Matrix class
 </summary>
	<remarks>
 Inherits List(Of List(Of Double))
 
 TODO:
 LU, Solve ,SVD
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction4.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction4.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction4">
	<summary>
 Benchmark function
 De Jong’s function 4 (qudratic with gauss Function)
 </summary>
	<remarks>
 Minimum:
  x = {0, ..., 0}
 Range
  -1.28 ~ 1.28
 Refference:
  De Jong, K. A., "Analysis of the Behavior of a Class of Genetic Adaptive Systems", PhD dissertation, The University of Michigan, Computer and Communication Sciences Department (1975)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchEasom.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchEasom.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchEasom">
	<summary>
 Benchmark function
 Easom function
 </summary>
	<remarks>
 Minimum:
  x = {pi, pi}
  f(x) = -1
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Double,System.Double,System.Double,System.Double)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default 5 =&gt; -5 to 5)</param>
	<param name="ai_maxIteration">Optional:Iteration(default 5000)</param>
	<param name="ai_eps">Optional:Eps(default:1e-6)</param>
	<param name="ai_coeffRefrection">Optional:Refrection coeffcient(default:1.0)</param>
	<param name="ai_coeffExpantion">Optional:Expantion coeffcient(default:2.0)</param>
	<param name="ai_coeffContraction">Optional:Contraction coeffcient(default:0.5)</param>
	<param name="ai_coeffShrink">Optional:Shrink coeffcient(default:2.0)</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.Init">
	<summary>
 Init
 </summary>
	<remarks>
 All vertexs are made at random.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.Init(System.Double[])">
	<summary>
 Init
 </summary>
	<param name="ai_initPoint">Set 1 vertex</param>
	<remarks>
 Other vertexs are made at random.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.Init(System.Double[][])">
	<summary>
 Init
 </summary>
	<param name="ai_initPoint"></param>
	<remarks>
 Set simplex
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.DoIteration(System.Int32)">
	<summary>
 Do optimization
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.Ame(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint)">
	<summary>
 Contraction, Expantion
 </summary>
	<param name="ai_base1"></param>
	<param name="ai_base2"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptNelderMeadNR.Result">
	<summary>
 Best result
 </summary>
	<returns>Best point class</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.GetCentroid(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint})">
	<summary>
 Calc Centroid
 </summary>
	<param name="ai_vertexs"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.Shrink(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint}@,System.Double)">
	<summary>
 Shrink(All point replace)
 </summary>
	<param name="ai_allVertexs"></param>
	<param name="ai_coeff">Shrink coeffcient.</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.IsConversion2(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint})">
	<summary>
 Check conversion
 </summary>
	<param name="ai_vertexs">All vertexs</param>
	<returns></returns>
	<remarks>
 Reffrence:
 William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
 "NUMRICAL RECIPIES 3rd Edition: The Art of Scientific Computing", Cambridge University Press 2007, pp505-506
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.GetLastErrorInfomation">
	<summary>
 Get recent error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptNelderMeadNR.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptNelderMeadNR.AllResult">
	<summary>
 All Result
 </summary>
	<value></value>
	<returns></returns>
	<remarks>
 for Debug, Experiment
 </remarks>
</member><member name="T:LibOptimization.Optimization.clsOptNelderMeadNR">
	<summary>
 Nelder Mead Method
 </summary>
	<remarks>
 Features:
  -Derivative free optimization algorithm.
  -Also known as "Down hill simplex" or "simplex method".
 
 Reffrence:
 William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
 "Numerical Recipes in C [日本語版] C言語による数値計算のレシピ", 平成19年第14刷, 株式会社技術評論社, pp295-299
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchAckley.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchAckley.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchAckley">
	<summary>
 Benchmark function
 Ackley's function
 </summary>
	<remarks>
 Features:
  -Multi modal.
 
 Minimum:
  x = {0,...,0}
 
 Refference:
 [1]小林重信, "実数値GAのフロンティア"，人工知能学会誌 Vol. 24, No. 1, pp.147-162 (2009)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction5.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction5.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchDeJongFunction5">
	<summary>
 Benchmark function
 De Jong’s function 5
 </summary>
	<remarks>
 Minimum:
  x = {-32,-32}
  f(x) ~ 1
 Range
  -65.536 ~ 65.536
 Refference:
  De Jong, K. A., "Analysis of the Behavior of a Class of Genetic Adaptive Systems", PhD dissertation, The University of Michigan, Computer and Communication Sciences Department (1975)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchSchafferFunction.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchSchafferFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchSchafferFunction">
	<summary>
 Benchmark function
 Schaffer function
 </summary>
	<remarks>
 Minimum:
  x = {0,...,0}
 
 Refference:
 [1]小林重信, "実数値GAのフロンティア"，人工知能学会誌 Vol. 24, No. 1, pp.147-162 (2009)
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Boolean,System.Int32,System.Int32)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default: 10 =&gt; -10 to 10)</param>
	<param name="ai_generation">Optional:Generation(Default: 10000)</param>
	<param name="ai_eps">Optional:Eps(Default:1e-8)</param>
	<param name="ai_isUseEps">Optional:Use criterion(Default: true)</param>
	<param name="ai_populationSize">Optional:Population size(0 is n*8)</param>
	<param name="ai_childsSize">Optional:Childs size(0 is n*6)</param>
	<remarks>
 "n" is function dimension.
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.Init">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.DoIteration(System.Int32)">
	<summary>
 Do Iteration
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.UseEliteStrategy(System.Double)">
	<summary>
 using Elite Strategy
 </summary>
	<param name="ai_density">density</param>
	<remarks>
 Elite strategy
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.SelectParent(System.Collections.Generic.List{LibOptimization.Optimization.clsPoint},System.Int32)">
	<summary>
 Select Parent
 </summary>
	<param name="ai_population"></param>
	<param name="ai_parentSize"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.CrossOverSPX(System.Int32,System.Collections.Generic.List{System.Collections.Generic.KeyValuePair{System.Int32,LibOptimization.Optimization.clsPoint}})">
	<summary>
 Simplex Crossover
 </summary>
	<param name="ai_childSize"></param>
	<param name="ai_parents"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptRealGASPX.Result">
	<summary>
 Best result
 </summary>
	<returns>Best point class</returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptRealGASPX.AllResult">
	<summary>
 All Result
 </summary>
	<value></value>
	<returns></returns>
	<remarks>
 for Debug, Experiment
 </remarks>
</member><member name="M:LibOptimization.Optimization.clsOptRealGASPX.IsCriterion(LibOptimization.Optimization.clsPoint,LibOptimization.Optimization.clsPoint)">
	<summary>
 Check Criterion
 </summary>
	<param name="ai_best"></param>
	<param name="ai_worst"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptRealGASPX">
	<summary>
 Real-coded Genetic Algorithm
 REX + JGG
 </summary>
	<remarks>
 Features:
  -Derivative free optimization algorithm.
  -Cross over algorithm is SPX(Simplex Cross over).
  -Alternation of generation algorithm is JGG.
 
 Refference:
 樋口 隆英, 筒井 茂義, 山村 雅幸, "実数値GAにおけるシンプレクス交叉", 人工知能学会論文誌Vol. 16 (2001) No. 1 pp.147-155
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRastriginFunction.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRastriginFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="ai_var"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchRastriginFunction">
	<summary>
 Benchmark function
 Rastrigin function
 </summary>
	<remarks>
 Minimum:
  x = {0,...,0}
 
 Referrence:
 http://mikilab.doshisha.ac.jp/dia/research/pdga/archive/doc/ga2k_performance.pdf
 </remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRosenblock.#ctor(System.Int32)">
	<summary>
 Default constructor
 </summary>
	<param name="ai_dim">Set dimension</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.BenchmarkFunction.clsBenchRosenblock.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Target Function
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.BenchmarkFunction.clsBenchRosenblock">
	<summary>
 Benchmark function
 Rosenblock function(Banana function)
 </summary>
	<remarks>
 Features:
  -Famous benchmark function.
 
 Minimum:
  x = {0,...,0}
 </remarks>
</member><member name="M:LibOptimization.Util.clsError.clsErrorInfomation.#ctor">
	<summary>
 Default constructor(do not use)
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.clsErrorInfomation.#ctor(System.Boolean,LibOptimization.Util.clsError.ErrorType,System.String)">
	<summary>
 Constructor
 </summary>
	<param name="ai_setError"></param>
	<param name="ai_errorType"></param>
	<param name="ai_errorMsg"></param>
	<remarks></remarks>
</member><member name="T:LibOptimization.Util.clsError.clsErrorInfomation">
	<summary>
 Error infomation class
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.SetError(System.Boolean,LibOptimization.Util.clsError.ErrorType,System.String)">
	<summary>
 Set Error
 </summary>
	<param name="ai_setError"></param>
	<param name="ai_errorType"></param>
	<param name="ai_errorMsg"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.IsError">
	<summary>
 Is error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.GetLastErrorInfomation">
	<summary>
 Get Last error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.Clear">
	<summary>
 Clear error
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsError.Print(LibOptimization.Util.clsError.clsErrorInfomation)">
	<summary>
 Error Output
 </summary>
	<remarks></remarks>
</member><member name="T:LibOptimization.Util.clsError">
	<summary>
 ErrorManage class
 </summary>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.Optimization.absObjectiveFunction.absFuncForOptimization">
	<summary>
 default constructor
 </summary>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.absObjectiveFunction.NumberOfVariable">
	<summary>
 Get number of variables
 </summary>
	<value></value>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.absObjectiveFunction.F(System.Collections.Generic.List{System.Double})">
	<summary>
 Evaluate
 </summary>
	<param name="x"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.absObjectiveFunction.Gradient(System.Collections.Generic.List{System.Double})">
	<summary>
 Gradient vector
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks>
 ex)
 f(x1,..,xn) = x1^2 + ... + xn^2
 del f =  [df/dx1 , ... , df/dxn]
 </remarks>
</member><member name="M:LibOptimization.Optimization.absObjectiveFunction.Hessian(System.Collections.Generic.List{System.Double})">
	<summary>
 Hessian matrix
 </summary>
	<param name="x"></param>
	<returns></returns>
	<remarks>
 ex)
 f(x1,x2) = x1^2 + x2^2
 del f   =  [df/dx1 df/dx2]
 del^2 f = [d^2f/d^2x1     d^2f/dx1dx2]
           [d^2f/d^2dx2dx1 d^2f/d^2x2]
 </remarks>
</member><member name="T:LibOptimization.Optimization.absObjectiveFunction">
	<summary>
 Abstarct objective function class
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.#ctor(System.UInt32)">
	<summary>
 Constructor with seed
 </summary>
	<param name="ai_seed"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.Sample">
	<summary>
 Sample
 </summary>
	<returns></returns>
	<remarks>
 The Sample method generates a distribution proportional to the value of the random numbers, in the range [0.0, 1.0].
 </remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.SetSeed(System.UInt32)">
	<summary>
 Set random seed
 </summary>
	<param name="ai_seed"></param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.RotateLeftShiftForUInteger(System.UInt32,System.Int32)">
	<summary>
 Rotate Shift
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.NextDouble(System.Double,System.Double)">
	<summary>
 Random double with range
 </summary>
	<param name="ai_min"></param>
	<param name="ai_max"></param>
	<returns></returns>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.GetTimeSeed">
	<summary>
 for random seed
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshift.Xor128">
	<summary>
 Xor128
 </summary>
	<returns></returns>
	<remarks>
 C Source by refference
  t=(xˆ(x leftshift 11));
  x=y;
  y=z;
  z=w;
  return( w=(wˆ(w rightshift 19))ˆ(tˆ(t rightshift 8)) )
 </remarks>
</member><member name="T:LibOptimization.Util.clsRandomXorshift">
	<summary>
 Xorshift random algorithm
 Inherits System.Random
 </summary>
	<remarks>
 Refference:
 George Marsaglia, "Xorshift RNGs", Journal of Statistical Software Vol. 8, Issue 14, Jul 2003
 </remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshiftSingleton.#ctor">
	<summary>
 Default constructor
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshiftSingleton.GetInstance">
	<summary>
 Instance
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsRandomXorshiftSingleton.SetSeed(System.UInt32)">
	<summary>
 Set Seed
 </summary>
	<param name="ai_seed"></param>
	<remarks></remarks>
</member><member name="T:LibOptimization.Util.clsRandomXorshiftSingleton">
	<summary>
 Xorshift random algorithm singleton
 </summary>
	<remarks>
	</remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.#ctor(LibOptimization.Optimization.absObjectiveFunction,System.Double,System.Int32,System.Double,System.Double)">
	<summary>
 Constructor
 </summary>
	<param name="ai_func">Optimize Function</param>
	<param name="ai_randomRange">Optional:random range(Default 5 =&gt; -5 to 5)</param>
	<param name="ai_maxIteration">Optional:Iteration(default 1000)</param>
	<param name="ai_eps">Optional:Eps(default:1e-8)</param>
	<param name="ai_alpha">Optinal:update alpha(default 0.3)</param>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.Init">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.Init(System.Double[])">
	<summary>
 Init
 </summary>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.DoIteration(System.Int32)">
	<summary>
 Do Iteration
 </summary>
	<param name="ai_iteration">Iteration count. When you set zero, use the default value.</param>
	<returns>True:Stopping Criterion. False:Do not Stopping Criterion</returns>
	<remarks></remarks>
</member><member name="P:LibOptimization.Optimization.clsOptSteepestDescent.Result">
	<summary>
 Result
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.GetLastErrorInfomation">
	<summary>
 Get recent error infomation
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Optimization.clsOptSteepestDescent.IsRecentError">
	<summary>
 Get recent error
 </summary>
	<returns></returns>
	<remarks></remarks>
</member><member name="T:LibOptimization.Optimization.clsOptSteepestDescent">
	<summary>
 Steepest descent method
 </summary>
	<remarks>
 Features:
  -Use first derivertive.
  -First order conversion.
 
 Refference:
 [1]http://dsl4.eee.u-ryukyu.ac.jp/DOCS/nlp/node4.html
 [2]金谷健一, "これならわかる最適化数学－基礎原理から計算手法まで－", 共立出版株式会社 2007 初版第7刷, pp79-84 
 
 Implment:
 N.Tomi(tomi.nori+github at gmail.com)
 
 Memo:
 最適化で微分を用いて求める手法のことを「勾配法」という。
 最大値を求めることを山登り法、最小値の場合は最急降下法とよばれる。
 </remarks>
</member><member name="M:LibOptimization.Util.clsUtil.NormRand(System.Double,System.Double)">
	<summary>
 Normal Distribution
 </summary>
	<param name="ai_ave">Average</param>
	<param name="ai_sigma2">Varianse s^2</param>
	<returns></returns>
	<remarks>
 using Box-Muller method
 </remarks>
</member><member name="M:LibOptimization.Util.clsUtil.RandomPermutaion(System.Int32)">
	<summary>
 Generate Random permutation
 </summary>
	<param name="ai_max"></param>
	<returns></returns>
	<remarks></remarks>
</member><member name="M:LibOptimization.Util.clsUtil.DebugValue(LibOptimization.Optimization.absOptimization,System.Int32,System.Boolean,System.Boolean)">
	<summary>
 For Debug
 </summary>
	<param name="ai_opt"></param>
	<param name="ai_precision"></param>
	<param name="ai_isOutValue"></param>
	<param name="ai_isOnlyIterationCount"></param>
	<remarks></remarks>
</member><member name="T:LibOptimization.Util.clsUtil">
	<summary>
 common use
 </summary>
	<remarks></remarks>
</member>
</members>
</doc>